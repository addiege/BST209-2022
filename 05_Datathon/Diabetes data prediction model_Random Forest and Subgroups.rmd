---
title: "Diabetes Mellitus Prediction Model"
 author: "Skyler Shapiro (original code author), Jennifer Cape, Shernaz Dossabhoy, Addison Gearhart and Po-Chih Kuo"
date: "2022/07/21"
output: 
  html_document: 
    highlight: haddock
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(gargle_verbosity = "info")
```

## 0. Introduction

This script presents our group project during the BST209 course. We will show our workflow for training and testing a diabetes mellitus prediction model in ICU patients. An outline for the project workflow is as follows:

1.  Load required packages and gather data
2.  Split the data into training and testing sets
3.  Preprocess the data and remove missing values
4.  Build a random forest classifier using the training set
5.  Evaluate our model on the test set

Project importance: Why is it important to predict diabetes mellitus in ICU patients?

In the hospital, when patients are admitted to the ICU, providers are often unaware of their past medical history. So, our group's mission was to build a model that can predict a diagnosis of diabetes mellitus in patients when they are first admitted to the ICU as this would improve clinical decision making and improve care of patients.

## 1. Getting started

### 1.1 Loaded required packages

First, we loaded the R packages necessary for our project.

```{r, echo = FALSE, include=FALSE}
# Checking that all required packages are installed
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("tableone")) install.packages("tableone")
if(!require("caret")) install.packages("caret")
if(!require("dplyr")) install.packages("dplyr")
if(!require("ROCR")) install.packages("ROCR")
if(!require("tidyverse")) install.packages("randomForest")
if(!require("knitr")) install.packages("knitr")

# List of packages to load
packages <- c("tidyverse", "tableone", "caret", "dplyr", "ROCR",
              "knitr", "randomForest")

# Load packages
lapply(packages, FUN = function(X) {
    do.call("require", list(X))
})
```

### 1.2 Loaded the dataset

Accessed the data from PhysioNet from the [WIDS Datathon Project Page](https://physionet.org/content/widsdatathon2020/). Downloaded 'training_v2.csv' from the "Files" section of the project.

Next we loaded in our data and started working with it.

```{r, echo = FALSE, include=FALSE}
# Read in data (might take a few seconds)
gossis <- read_csv("~/Downloads/training_v2.csv")
```

### 1.3 Subset selection

We included eight predictors in our model: `age`, `bmi`, `gender`, `ethnicity`, `glucose apache`, `H1 glucose max`, `D1 glucose max`, and `WBC apache`. 

```{r}
#  Select subset of variables from original data
reduced_data <- gossis %>% select(bmi,
                                  age,
                                  gender,
                                  glucose_apache,
                                  diabetes_mellitus, h1_glucose_max, d1_glucose_max,ethnicity,wbc_apache)
head(reduced_data) %>% kable()
```

We also defined our outcome variable and removed the data with unknown outcomes

```{r set_outcome, echo=TRUE}
# Set the outcome variable here
reduced_data$outcome_variable <- as.factor(reduced_data$diabetes_mellitus)
reduced_data <- subset(reduced_data, select = -c(diabetes_mellitus))

# Check number of rows
nrow(reduced_data)

# For simplicity, we dropped all rows with missing outcomes
reduced_data <- drop_na(reduced_data, any_of("outcome_variable"))

# Check number of rows after removing rows with missing outcomes
nrow(reduced_data)
```

## 2. Creating the training and testing sets

### 2.1 Encoding

Encoding our categorical variables. 

```{r}
# Encode gender variable: male = 1, non-male = 0
reduced_data$gender <- ifelse(reduced_data$gender == "M", 1, 0)
```

### 2.2 Splitting the data

Creating the training and test datasets.

```{r training_test, echo=TRUE}
# Setting the random number seed for reproducibility
set.seed(1)

# Creating data partition using the outcome variable
train_index <- createDataPartition(reduced_data$outcome_variable,
                                   times = 1, p = 0.8, list = FALSE)

# Splitting data into train and test sets, selected columns that will be used in model
train_set <- reduced_data[train_index, ]
head(train_set)
test_set <- reduced_data[-train_index, ]
head(test_set)
```

## 3. Summary statistics ("Table 1")

-   Our summary statistics are provided as Table 1 using `library(tableone)` 
-   Developed by Dr Kazuki Yoshida, while a Ph.D. student at Harvard University.
-   [TableOne documentation](https://cran.r-project.org/web/packages/tableone/tableone.pdf)

```{r tableone, echo=TRUE}
allvars <- c("bmi", "age", "gender", "glucose_apache", "h1_glucose_max", "d1_glucose_max","ethnicity", "wbc_apache")

catvars <- c("gender")

table_gossis <- CreateTableOne(vars = allvars, data = train_set,
                factorVars = catvars,
                strata = "outcome_variable")

kableone(table_gossis, caption = "Summary statistics of the training set")
```

## 4. Preprocessing the data

### 4.1 Imputing missing values

To deal with the NA values in our data, we decided to replace missing values with the median of the train_set.

Note that it is important that we do this *after* splitting into training and test sets. If we imputed values using test data, we would risk leaking information about our test set into our training set ("data leakage").

```{r preprocessing, echo=TRUE}

predictors <- c("age", "bmi", "gender", "glucose_apache", "h1_glucose_max", "d1_glucose_max", "ethnicity", "wbc_apache")

for (col in predictors) {
    test_set[[col]][is.na(test_set[[col]])] <-
        median(train_set[[col]], na.rm = TRUE)
    train_set[[col]][is.na(train_set[[col]])] <-
        median(train_set[[col]], na.rm = TRUE)
}
```

### 4.2 Normalization

Since our tree model does not require normalization, so we skipped this step.

## 5. Model building

### 5.1 Training a random forest classifier

-   We used the `randomForest` library to build a random forest model.
-   See the [randomForest documentation on CRAN](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf).


### Changing the model is also possible and we have provided documentation in separate notbeooks for this. http://topepo.github.io/caret/train-models-by-tag.html
for example: metric = "svmPoly", metric = "glm"

```{r train_model, echo=TRUE}
# Define forest tuning parameters
# 5-fold cross validation
control <- trainControl(method = "repeatedcv", number = 2)
# Number of variables tried at each split
mtry <- sqrt(ncol(train_set))
# Grid search is a linear search through a vector of mtry values
tunegrid <- expand.grid(.mtry = mtry)
# Create classification forest using age, bmi, and gender
forest <- train(outcome_variable ~ age + bmi + gender + glucose_apache + h1_glucose_max + d1_glucose_max + ethnicity + wbc_apache,
              data = train_set,
              method = "rf",
              metric = "Accuracy",
              tuneGrid = tunegrid,
              trControl = control)
```

### 5.2 Viewing variable importance

-   We used `caret` for variable importance [Documentation](https://topepo.github.io/caret/variable-importance.html)

```{r importance, echo=TRUE}
# Calculate variable importance
importance <- varImp(forest)
kable(importance$importance)
```

We can see that `d1_glucose_max` and `glucose_apache` had the most predictive power, followed by `bmi`.

### 5.3 Predicting diabetes mellitus in "unseen patients"

#### Selecting patients from the test set

-   Let's look at some patients in our test set

```{r select_unseen, echo=TRUE}
head(test_set, 5) %>% select(age, bmi, gender, glucose_apache, h1_glucose_max, d1_glucose_max, ethnicity, wbc_apache)
```

```{r}
original_test_set <- test_set
```


```{r}
install.packages("Hmisc")
library(Hmisc)
hist.data.frame(original_test_set)
```


#### Predicting diabetes mellitus in unseen patients
####Original results:

```{r predict_unseen, echo=TRUE}
# Make predictions on test set

test_set <- original_test_set

forest_pred <- predict(forest,
                       newdata = test_set,
                       type = "raw")

# Combine unseen patients data with corresponding predictions
data.frame(age = test_set$age,
           bmi = test_set$bmi,
           gender = test_set$gender,
           glucose_apache = test_set$glucose_apache,
           h1_glucose_max = test_set$h1_glucose_max, 
           d1_glucose_max = test_set$d1_glucose_max,
           ethnicity = test_set$ethnicity,
           wbc_apache = test_set$wbc_apache,
           prediction = forest_pred,
           truth_value = test_set$outcome_variable) %>%
          head(30) %>%
          kable()

```


```{r AUROC, echo=TRUE}
# Get the probabilities for our forest predictions
forest_probs <- predict(forest, newdata = test_set, type = "prob")
forest_probs <- forest_probs[, 2]
# Create a "prediction" object using our probabilities and the outcome variable
forest_roc <- prediction(forest_probs, test_set$outcome_variable)
forest_perf <- performance(forest_roc, measure = "tpr", x.measure = "fpr")
# Plot the ROC curve
plot(forest_perf, col = rainbow(10))
abline(a = 0, b = 1)
# Calculate the AUC
auc_forest <- performance(forest_roc, measure = "auc")
auc_forest <- auc_forest@y.values[[1]]
print(auc_forest)
```

### A dataframe to save results
```{r}
df_result <- data.frame(name = character(), auc = double(), range = character())
```

#### Subgroup analysis (discrete variables):

```{r}
var_list <- c("gender", "ethnicity")

for (var in var_list)
{
print(var)
    
list <- unique(original_test_set[[var]])

for (i in list)
{
test_set <- original_test_set[original_test_set[[var]] == i, ]
num = dim(test_set)[1]

# Make predictions on test set
forest_pred <- predict(forest,
                       newdata = test_set,
                       type = "raw")

data.frame(age = test_set$age,
           bmi = test_set$bmi,
           gender = test_set$gender,
           glucose_apache = test_set$glucose_apache,
           h1_glucose_max = test_set$h1_glucose_max, 
           d1_glucose_max = test_set$d1_glucose_max,
           ethnicity = test_set$ethnicity,
           wbc_apache = test_set$wbc_apache,
           prediction = forest_pred,
           truth_value = test_set$outcome_variable) 

  
# Get the probabilities for our forest predictions
forest_probs <- predict(forest, newdata = test_set, type = "prob")
forest_probs <- forest_probs[, 2]
# Create a "prediction" object using our probabilities and the outcome variable
forest_roc <- prediction(forest_probs, test_set$outcome_variable)
forest_perf <- performance(forest_roc, measure = "tpr", x.measure = "fpr")
# Plot the ROC curve
plot(forest_perf, col = rainbow(10), main= paste(var,":",i,"(",num,")"))
abline(a = 0, b = 1)
# Calculate the AUC
auc_forest <- performance(forest_roc, measure = "auc")
auc_forest <- auc_forest@y.values[[1]]
print(paste(i,"(", num, "):", auc_forest))
df_result <- rbind(df_result, data.frame(name = var, auc = auc_forest, range = i))

}
}
  
```
#### Subgroup analysis (continuous variables):

```{r}
df <- data.frame(name = "age", range = c(20, 40, 60, 80))
df <- rbind(df, data.frame(name = "bmi", range = c(20, 30, 40)))
df <- rbind(df, data.frame(name = "glucose_apache", range = c(100, 200, 300)))
df <- rbind(df, data.frame(name = "h1_glucose_max", range = c(100, 200, 300)))
df <- rbind(df, data.frame(name = "d1_glucose_max", range = c(100, 200, 300)))
df <- rbind(df, data.frame(name = "wbc_apache", range = c(5, 10, 15, 20)))
```

```{r}

var_list <- c("age","bmi","glucose_apache", "h1_glucose_max", "d1_glucose_max", "wbc_apache")

for (var in var_list)
{
print(var)
var_range = df[df$name==var,"range"]

list <- unique(original_test_set[[var]])

for (i in 0:length(var_range)+1)
{
  
if(i==1){
  range_text = paste("<", var_range[i], "(",num, ")")
}
else if(i==length(var_range)+1){
  range_text = paste(">", var_range[i-1], "(",num, ")")
}
else{
  range_text = paste(var_range[i-1],"~", var_range[i], "(",num, ")")
}
  
  
if(i==1){
  test_set <- original_test_set[original_test_set[[var]] < var_range[i], ]
}
else if(i==length(var_range)+1){
  test_set <- original_test_set[original_test_set[[var]] > var_range[i-1], ]
}
else{
  test_set <- original_test_set[original_test_set[[var]] > var_range[i-1] & original_test_set[[var]] < var_range[i], ]
}

num = dim(test_set)[1]
  
# Make predictions on test set
forest_pred <- predict(forest,
                       newdata = test_set,
                       type = "raw")

data.frame(age = test_set$age,
           bmi = test_set$bmi,
           gender = test_set$gender,
           glucose_apache = test_set$glucose_apache,
           h1_glucose_max = test_set$h1_glucose_max, 
           d1_glucose_max = test_set$d1_glucose_max,
           ethnicity = test_set$ethnicity,
           wbc_apache = test_set$wbc_apache,
           prediction = forest_pred,
           truth_value = test_set$outcome_variable) 

  
# Get the probabilities for our forest predictions
forest_probs <- predict(forest, newdata = test_set, type = "prob")
forest_probs <- forest_probs[, 2]
# Create a "prediction" object using our probabilities and the outcome variable
forest_roc <- prediction(forest_probs, test_set$outcome_variable)
forest_perf <- performance(forest_roc, measure = "tpr", x.measure = "fpr")
# Plot the ROC curve
plot(forest_perf, col = rainbow(10), main= paste(var,":",range_text))
abline(a = 0, b = 1)
# Calculate the AUC
auc_forest <- performance(forest_roc, measure = "auc")
auc_forest <- auc_forest@y.values[[1]]

print(paste(range_text,":", auc_forest))

df_result <- rbind(df_result, data.frame(name = var, auc = auc_forest, range = range_text))

}
}
  
```

```{r}
var_list <- c("gender","ethnicity","age","bmi","glucose_apache", "h1_glucose_max", "d1_glucose_max", "wbc_apache")

for (var in var_list)
{
tmp <- df_result %>% filter(name == var)
print(ggplot(tmp, aes(range, auc)) + 
  xlab(var) + ylab("AUC") +
  geom_bar(stat = "identity"))
}
```

## 6. Model evaluation

### 6.1 Creating our confusion matrix

-   A confusion matrix relates predictions to the ground truth.
-   Forms the basis for evaluation metrics.
-   Non-diabetes is our "0" (-ve). Diabetes is our "1" (+ve).

```{r create_confusion_matrix, echo=TRUE}
# Ground truth is recorded in the GOSSIS data
cm = confusionMatrix(forest_pred,
                test_set$outcome_variable,
                positive = "1")$tab
cm
```
```{r}
TClass <- factor(c(0, 0, 1, 1))
PClass <- factor(c(0, 1, 0, 1))
Y      <- c(cm[1], cm[2], cm[3], cm[4])
df_2 <- data.frame(TClass, PClass, Y)

library(ggplot2)
ggplot(data =  df_2, mapping = aes(x = TClass, y = PClass)) +
  geom_tile(aes(fill = Y), colour = "white") +
  geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_bw() + theme(legend.position = "none")
```

### 6.2 Evaluation metrics

#### 6.2.1 Accuracy

-   How many shots did we take? What proportion hit the target?
-   Accuracy = $\frac{(TP + TN)}{(TP + TN + FP + FN)}$

#### 6.2.2 Sensitivity

-   (AKA "Recall" AKA "True Positive Rate")
-   Recall = $\frac{(TP)}{(TP + FN)}$

#### 6.2.1 Specificity

-   (AKA "True Negative Rate")
-   Specificity = $\frac{(TN)}{(TN + FP)}$

#### 6.2.1 Area under ROC Curve (AUC)

-   Developed in the 1940s by radar operators.
-   Popular measure of discrimination.
-   Plots *1 - specificity* vs. *sensitivity* at varying probability thresholds.
-   0.5 is terrible. 1.0 is perfect.
-   AUC of 0.9 tells us that the 90% of time our model will assign a higher risk to a randomly selected patient with an event than to a randomly selected patient without an event

We'll use the [ROCR package](https://cran.r-project.org/web/packages/ROCR/index.html) ("Visualizing the Performance of Scoring Classifiers") to plot the curve.



# Additional steps. In addition to adding predictors and evaluating performance in patient subgroups, we built different model types, including:

-   Decision Tree 
-   Logistic Regression
-   XGboost
-   SVM

Please see attached separate R Markdown notebooks.