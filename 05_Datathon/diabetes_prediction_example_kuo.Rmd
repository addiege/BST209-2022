---
title: "Diabetes Mellitus Prediction"
author: "Skyler Shapiro"
date: "2022/06/21"
output: 
  html_document: 
    highlight: haddock
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(gargle_verbosity = "info")
```

## 0. Introduction

This script runs through a simple workflow for training and testing a diabetes mellitus prediction model. 
Briefly, we will:

1. Load required packages and gather data
2. Split the data into training and testing sets
3. Preprocess the data and remove missing values 
4. Build a random forest classifier using the training set
5. Evaluate our model on the test set

You may ask: Why is it important to predict cases of diabetes mellitus in patients?

In the hospital, patient medical records may take days to transfer. Knowledge about chronic conditions like diabetes can inform clinical decisions about patient care and ultimately improve patient outcomes.

Lets begin!

## 1. Getting started

### 1.1 Load required packages 

First, let's load the R packages necessary for our project.

```{r, echo = FALSE, include=FALSE}
# Checking that all required packages are installed
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("tableone")) install.packages("tableone")
if(!require("caret")) install.packages("caret")
if(!require("dplyr")) install.packages("dplyr")
if(!require("ROCR")) install.packages("ROCR")
if(!require("tidyverse")) install.packages("randomForest")
if(!require("knitr")) install.packages("knitr")

# List of packages to load
packages <- c("tidyverse", "tableone", "caret", "dplyr", "ROCR",
              "knitr", "randomForest")

# Load packages
lapply(packages, FUN = function(X) {
    do.call("require", list(X))
})
```


### 1.2 Load the dataset

Access the data from PhysioNet from the [WIDS Datathon Project Page](https://physionet.org/content/widsdatathon2020/). 
Download 'training_v2.csv' from the "Files" section of the project.

Now we can load in our data and start to work with it.

```{r, echo = FALSE, include=FALSE}
# Read in data (might take a few seconds)
gossis <- read_csv("~/Downloads/training_v2.csv")
```

```{r}
# Check dimensions of data
# Output format:  rows  by columns
dim(gossis)
``` 

We can see our data has `r dim(gossis)[1]` entries and `r dim(gossis)[2]` Variables. Let's see what our data actually looks like!

```{r}
head(gossis[, 1:7]) %>% kable(caption = "GOSSIS (1st seven variables)")
```

### 1.3 Subset selection

We will include four predictors in our model: `age`, `bmi`, `gender`, and `glucose apache`. Feel free to include more or choose entirely different predictors in your model!

```{r}
#  Select subset of variables from original data
reduced_data <- gossis %>% select(bmi,
                                  age,
                                  gender,
                                  glucose_apache,
                                  diabetes_mellitus)
head(reduced_data) %>% kable()
```

We will also define our outcome variable and drop data with unknown outcomes

```{r set_outcome, echo=TRUE}
# Set the outcome variable here
reduced_data$outcome_variable <- as.factor(reduced_data$diabetes_mellitus)
reduced_data <- subset(reduced_data, select = -c(diabetes_mellitus))

# Check number of rows
nrow(reduced_data)

# For simplicity, we will drop all rows with missing outcomes
reduced_data <- drop_na(reduced_data, any_of("outcome_variable"))

# Check number of rows after removing rows with missing outcomes
nrow(reduced_data)
```

## 2. Create the training and testing sets

### 2.1 Encoding

We'll encode our categorical variables. Encoding is the process of reshaping and binarizing categorical data to better suit machine learning models.


```{r}
# Encode gender variable: male = 1, non-male = 0
reduced_data$gender <- ifelse(reduced_data$gender == "M", 1, 0)
```

### 2.2 Split the data

Let's create the training and test datasets.

```{r training_test, echo=TRUE}
# Set the random number seed for reproducibility
set.seed(1)

# Create data partition using the outcome variable
train_index <- createDataPartition(reduced_data$outcome_variable,
                                   times = 1, p = 0.8, list = FALSE)

# Split data into train and test sets, select columns that will be used in model
train_set <- reduced_data[train_index, ]
head(train_set)
test_set <- reduced_data[-train_index, ]
head(test_set)
```

## 3. View summary statistics ("Table 1")

- Most studies include summary statistics as Table 1.
- `library(tableone)` makes it easy to create these summary statistics
- Developed by Dr Kazuki Yoshida, while a Ph.D. student at Harvard University.
- [TableOne documentation](https://cran.r-project.org/web/packages/tableone/tableone.pdf)

```{r tableone, echo=TRUE}
allvars <- c("bmi", "age", "gender", "glucose_apache")
catvars <- c("gender")

table_gossis <- CreateTableOne(vars = allvars, data = train_set,
                factorVars = catvars,
                strata = "outcome_variable")

kableone(table_gossis, caption = "Summary statistics of the training set")
```

## 4. Preprocessing the data

### 4.1 Impute missing values

We have to consider the NA values in our data. For simplicity, we will replace missing values with the median of the train_set.

Note that it is important that we do this _after_ splitting into training and test sets. If we imputed values using test data, we would risk leaking information about our test set into our training set ("data leakage").

Feel free to experiment with different imputation techniques!

```{r preprocessing, echo=TRUE}

predictors <- c("age", "bmi", "gender", "glucose_apache")

for (col in predictors) {
    test_set[[col]][is.na(test_set[[col]])] <-
        median(train_set[[col]], na.rm = TRUE)
    train_set[[col]][is.na(train_set[[col]])] <-
        median(train_set[[col]], na.rm = TRUE)
}
```

### 4.2 Normalization

For many models it is important to normalize the data. Without normalization, variables with larger magnitudes may have a greater effect on the model.

One common approach for normalization is to divide each variable by its standard deviation and subtract the mean. As before, to avoid data leakage, normalization should be done after splitting into training and test sets.

Our tree model does not require normalization, so we will skip this step.

## 5. Model building

### 5.1 Train a random forest classifier

- We will use the `randomForest` library to build a random forest model.
- See the [randomForest documentation on CRAN](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf).

```{r train_model, echo=TRUE}
# Define forest tuning parameters
# 5-fold cross validation
control <- trainControl(method = "repeatedcv", number = 2)
# Number of variables tried at each split
mtry <- sqrt(ncol(train_set))
# Grid search is a linear search through a vector of mtry values
tunegrid <- expand.grid(.mtry = mtry)
# Create classification forest using age, bmi, and gender
forest <- train(outcome_variable ~ age + bmi + gender + glucose_apache,
              data = train_set,
              method = "rf",
              metric = "Accuracy",
              tuneGrid = tunegrid,
              trControl = control)
```

### 5.2 View variable importance

- Many approaches for computing "importance".
- Shapley Additive exPlanations (ShAP) values are a popular alternative.
- Compute the marginal contribution of a feature across a combination of features.
- We will use `caret` variable importance [Documentation](https://topepo.github.io/caret/variable-importance.html)

```{r importance, echo=TRUE}
# Calculate variable importance
importance <- varImp(forest)
kable(importance$importance)
```

We can see that `glucose_apache` and `bmi` had the most predictive power.

### 5.3 Predict diabetes mellitus in "unseen patients"

#### Select patients from the test set

- Let's look some patients in our test set
- What status do you expect for these patients?

```{r select_unseen, echo=TRUE}
head(test_set, 5) %>% select(age, bmi, gender, glucose_apache)
```

#### Predict diabetes mellitus in unseen patients

- Is the model correct for this patient?
- `type="prob"` to output probabilities
- `type="raw"` to output classes

```{r predict_unseen, echo=TRUE}
# Make predictions on test set
forest_pred <- predict(forest,
                       newdata = test_set,
                       type = "raw")

# Combine unseen patients data with corresponding predictions
data.frame(age = test_set$age,
           bmi = test_set$bmi,
           gender = test_set$gender,
           glucose_apache = test_set$glucose_apache,
           prediction = forest_pred,
           truth_value = test_set$outcome_variable) %>%
          head(30) %>%
          kable()
```

## 6. Model evaluation

### 6.1 Creating our confusion matrix

- A confusion matrix relates predictions to the ground truth.
- Forms the basis for evaluation metrics.
- Non-diabetes is our "0" (-ve). Diabetes is our "1" (+ve).

```{r create_confusion_matrix, echo=TRUE}
# Ground truth is recorded in the GOSSIS data
confusionMatrix(forest_pred,
                test_set$outcome_variable,
                positive = "1")$tab
```

### 6.2 Evaluation metrics

#### 6.2.1 Accuracy

- How many shots did we take? What proportion hit the target?
- Accuracy = $\frac{(TP + TN)}{(TP + TN + FP + FN)}$

#### 6.2.2 Sensitivity

- (AKA "Recall" AKA "True Positive Rate")
- Recall = $\frac{(TP)}{(TP + FN)}$

#### 6.2.1 Specificity 

- (AKA "True Negative Rate")
- Specificity = $\frac{(TN)}{(TN + FP)}$

#### 6.2.1 Area under ROC Curve (AUC)

- Developed in the 1940s by radar operators.
- Popular measure of discrimination.
- Plots *1 - specificity* vs. *sensitivity* at varying probability thresholds.
- 0.5 is terrible. 1.0 is perfect.
- AUC of 0.9 tells us that the 90% of time our model will assign a higher risk to a randomly selected patient with an event than to a randomly selected patient without an event

We'll use the [ROCR package](https://cran.r-project.org/web/packages/ROCR/index.html) ("Visualizing the Performance of Scoring Classifiers") to plot the curve.

```{r AUROC, echo=TRUE}
# Get the probabilities for our forest predictions
forest_probs <- predict(forest, newdata = test_set, type = "prob")
forest_probs <- forest_probs[, 2]
# Create a "prediction" object using our probabilities and the outcome variable
forest_roc <- prediction(forest_probs, test_set$outcome_variable)
forest_perf <- performance(forest_roc, measure = "tpr", x.measure = "fpr")
# Plot the ROC curve
plot(forest_perf, col = rainbow(10))
abline(a = 0, b = 1)
# Calculate the AUC
auc_forest <- performance(forest_roc, measure = "auc")
auc_forest <- auc_forest@y.values[[1]]
print(auc_forest)
```

# Over to you!

There are many ways to improve the analysis. For example: 

- Add predictors
- Perform imputation on missing values
- Train a more robust model
- Change the prediction target
- Evaluate model calibration
- Evaluate performance in patient subgroups
